
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.4">
    
    
      
        <title>Training and Serving ML Models on GPU with FuseML & NVIDIA Triton - FuseML Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.db9e7362.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#4cae4f">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="green" data-md-color-accent="cyan">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#training-and-serving-ml-models-on-gpu-with-fuseml-and-nvidia-triton" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="FuseML Documentation" class="md-header__button md-logo" aria-label="FuseML Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            FuseML Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training and Serving ML Models on GPU with FuseML & NVIDIA Triton
            
          </span>
        </div>
      </div>
    </div>
    
    
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fuseml/fuseml/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="FuseML Documentation" class="md-nav__button md-logo" aria-label="FuseML Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    FuseML Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fuseml/fuseml/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../quickstart/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorials" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../kfserving-basic/" class="md-nav__link">
        Logistic Regression with MLFlow & KFServing
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Training and Serving ML Models on GPU with FuseML & NVIDIA Triton
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Training and Serving ML Models on GPU with FuseML & NVIDIA Triton
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#setup" class="md-nav__link">
    Setup
  </a>
  
    <nav class="md-nav" aria-label="Setup">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-the-kubernetes-cluster" class="md-nav__link">
    Creating the Kubernetes Cluster
  </a>
  
    <nav class="md-nav" aria-label="Creating the Kubernetes Cluster">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-gpu-accelerated-docker-containers" class="md-nav__link">
    Running GPU accelerated Docker containers
  </a>
  
    <nav class="md-nav" aria-label="Running GPU accelerated Docker containers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-nvidia-docker" class="md-nav__link">
    Install NVIDIA Docker
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build-custom-k3s-image" class="md-nav__link">
    Build custom k3s image
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-the-kubernetes-cluster" class="md-nav__link">
    Create the Kubernetes cluster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-fuseml" class="md-nav__link">
    Install FuseML
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#validating-the-model" class="md-nav__link">
    Validating the model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-and-serving-the-model-on-gpu" class="md-nav__link">
    Training and serving the model on GPU
  </a>
  
    <nav class="md-nav" aria-label="Training and serving the model on GPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cleanup" class="md-nav__link">
    Cleanup
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../architecture/" class="md-nav__link">
        Architecture
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../api/" class="md-nav__link">
        API Reference
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../cli/" class="md-nav__link">
        CLI Reference
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#setup" class="md-nav__link">
    Setup
  </a>
  
    <nav class="md-nav" aria-label="Setup">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-the-kubernetes-cluster" class="md-nav__link">
    Creating the Kubernetes Cluster
  </a>
  
    <nav class="md-nav" aria-label="Creating the Kubernetes Cluster">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-gpu-accelerated-docker-containers" class="md-nav__link">
    Running GPU accelerated Docker containers
  </a>
  
    <nav class="md-nav" aria-label="Running GPU accelerated Docker containers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-nvidia-docker" class="md-nav__link">
    Install NVIDIA Docker
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build-custom-k3s-image" class="md-nav__link">
    Build custom k3s image
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-the-kubernetes-cluster" class="md-nav__link">
    Create the Kubernetes cluster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-fuseml" class="md-nav__link">
    Install FuseML
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#validating-the-model" class="md-nav__link">
    Validating the model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-and-serving-the-model-on-gpu" class="md-nav__link">
    Training and serving the model on GPU
  </a>
  
    <nav class="md-nav" aria-label="Training and serving the model on GPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cleanup" class="md-nav__link">
    Cleanup
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    Conclusion
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fuseml/docs/edit/main/docs/tutorials/kfserving-triton-gpu.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="training-and-serving-ml-models-on-gpu-with-fuseml-and-nvidia-triton">Training and Serving ML Models on GPU with FuseML and NVIDIA Triton</h1>
<h2 id="introduction">Introduction</h2>
<p>Data scientists or machine learning engineers who looks to train models at scale with good performance eventually hit a point where they start to experience various degrees of slowness on the process. For example, tasks that usually were taking minutes to complete are now taking hours, or even days when datasets get larger. Such situation is particularly common when training Deep Learning models, for neural networks the training phase is the most resource-intensive task. While training, a neural network takes in inputs which are then processed in hidden layers using weights that are adjusted during training and the model then outputs a prediction. Weights are adjusted to find patterns in order to make better predictions. Both operations are essentially composed by matrix multiplications.</p>
<p>Taking in consideration a neural network with around 10, 100 or even 100,000 parameters, a computer would still be able to handle this in a matter of minutes, or even hours at the most.
However a neural network with more than 10 billion parameters would probably take years to train on the same system.</p>
<p>To overcome that problem GPUs (Graphics Processing Unit) are usually used for training neural networks. GPUs are optimized for training artificial intelligence and deep learning models as they can process multiple computations simultaneously. Additionally, GPUs have their own dedicated memory which allows it to compute huge amounts of data with higher bandwidth resulting in the training phase being much faster.</p>
<p>In this tutorial, we will explore how to use FuseML together with NVIDIA triton to not only train a neural network, but also serve it in a timely manner by taking advantage of the GPU. We will start by using k3d to create a local kubernetes cluster with two nodes exposing the the GPU to one of them. After that, we will install FuseML then create and execute a workflow that trains the model and then serves it, as a validation step we will not be using the GPU yet. Once the model is validated, we will increase its accuracy by increasing number of epochs used to train the model, by doing that it is expected that the training time would increase considerably, however as the training will be performed on GPU it will be rather quick.</p>
<h2 id="setup">Setup</h2>
<h3 id="creating-the-kubernetes-cluster">Creating the Kubernetes Cluster</h3>
<p>In order to run FuseML locally, we will be using <a href="https://k3d.io/">k3d</a>. K3d is a lightweight wrapper to run k3s (Rancher Labâ€™s minimal Kubernetes distribution) in docker making it very easy to create single and multi-node k3s clusters. As k3d uses docker to create the kubernetes nodes running containerd, we need to be able to run GPU accelerated Docker containers so that it can expose the GPU to containerd. Additionally, we also need to build a custom k3s image for the k3d cluster, this custom image basically contains the the NVIDIA Container Runtime and also configures containerd to use that runtime.</p>
<h4 id="running-gpu-accelerated-docker-containers">Running GPU accelerated Docker containers</h4>
<p>Running GPU accelerated docker containers can be done by using <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a>. Before getting started, ensure that your system meets the following requirements:
    * GNU/Linux x86_64 with kernel version &gt; 3.10
    * Docker &gt;= 19.03 (recommended, but some distributions may include older versions of Docker. The minimum supported version is 1.12)
    * NVIDIA GPU with Architecture &gt;= Kepler (or compute capability 3.0)
    * NVIDIA Linux drivers &gt;= 418.81.07 (Note that older driver releases or branches are unsupported.)</p>
<h5 id="install-nvidia-docker">Install NVIDIA Docker</h5>
<p>The instructions provided here are for OpenSUSE Leap 15.3. For other supported distributions please refer to the <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">official documentation</a>.</p>
<ol>
<li>Install the NVIDIA driver:
The recommended way to install drivers is to use the package manager for your distribution, however as the driver from the package manager did not work properly for OpenSUSE, we will be using the driver from the <a href="https://www.nvidia.com/Download/index.aspx?lang=en-us">NVIDIA website</a>. We will be installing the NVIDIA driver version 470.74, please check for the latest available driver compatible with your GPU on the <a href="https://www.nvidia.com/Download/index.aspx?lang=en-us">NVIDIA website</a>.</li>
</ol>
<p>Installing the driver:</p>
<pre><code class="language-bash">$ wget https://us.download.nvidia.com/XFree86/Linux-x86_64/470.74/NVIDIA-Linux-x86_64-470.74.run
$ chmod +x NVIDIA-Linux-x86_64-470.74.run
$ ./NVIDIA-Linux-x86_64-470.74.run
</code></pre>
<p>After installing the driver make sure to reboot your system.</p>
<ol>
<li>Install Docker:
Run the following command to install Docker:</li>
</ol>
<pre><code class="language-bash">$ sudo zypper install docker
</code></pre>
<ol>
<li>Add your user to the docker group.</li>
</ol>
<pre><code class="language-bash">$ sudo usermod -aG docker $USER
</code></pre>
<p>Log out and log back in so that your group membership is re-evaluated.</p>
<ol>
<li>Ensure Docker service is running and test your Docker installation by running the hello-world container and checking its output:</li>
</ol>
<pre><code class="language-bash">$ sudo systemctl --now enable docker
$ docker run --rm hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
2db29710123e: Pull complete
Digest: sha256:37a0b92b08d4919615c3ee023f7ddb068d12b8387475d64c622ac30f45c29c51
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
</code></pre>
<ol>
<li>Install nvidia-docker
The nvidia-docker package is available through a nvidia repository.
Add the repository and install it by running the following commands:</li>
</ol>
<pre><code class="language-bash">$ sudo zypper ar -f https://nvidia.github.io/nvidia-docker/opensuse-leap15.1/nvidia-docker.repo
$ sudo zypper install nvidia-docker2
</code></pre>
<p>When installing nvidia-docker it will overwrite the <code>/etc/docker/daemon.json</code> file by configuring docker to use the NVIDIA container runtime.
Make sure that it has the following contents:</p>
<pre><code class="language-bash">$ cat /etc/docker/daemon.json
{
    &quot;runtimes&quot;: {
        &quot;nvidia&quot;: {
            &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
            &quot;runtimeArgs&quot;: []
        }
    }
}
</code></pre>
<ol>
<li>Restart the Docker daemon to complete the installation and test it by running a CUDA container:</li>
</ol>
<pre><code class="language-bash">$ sudo systemctl restart docker
$ docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
Fri Oct 15 13:13:50 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   43C    P5    24W / 280W |      0MiB / 11175MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<h4 id="build-custom-k3s-image">Build custom k3s image</h4>
<p>The native K3s image is based on Alpine but the NVIDIA container runtime is not supported by it. To get around that we need to build the image using a supported base image.
The following instructions are based on the <a href="https://k3d.io/v5.0.0/usage/advanced/cuda/">official k3d documentation</a></p>
<ol>
<li>Create a Dockerfile with the content below. This Dockerfile uses nvidia/cuda as base image and installs the nvidia container runtime on it, adds a configuration file for containerd configuring it to use the NVIDIA container runtime and adds the NVIDIA device plugin daemonset so it can start automatically when creating the kubernetes cluster.</li>
</ol>
<pre><code class="language-Dockerfile">ARG K3S_TAG=&quot;v1.21.5-k3s1&quot;
ARG CUDA_VERSION=&quot;11.4.2&quot;

FROM rancher/k3s:$K3S_TAG as k3s

FROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04

RUN echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections

RUN apt-get update &amp;&amp; \
    apt-get -y install gnupg2 curl

# Install NVIDIA Container Runtime
RUN curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | apt-key add -

RUN curl -s -L https://nvidia.github.io/nvidia-container-runtime/ubuntu20.04/nvidia-container-runtime.list | tee /etc/apt/sources.list.d/nvidia-container-runtime.list

RUN apt-get update &amp;&amp; \
    apt-get -y install nvidia-container-runtime

COPY --from=k3s /bin /bin/

RUN mkdir -p /etc &amp;&amp; \
    echo 'hosts: files dns' &gt; /etc/nsswitch.conf

RUN chmod 1777 /tmp

# Provide custom containerd configuration to configure the nvidia-container-runtime
RUN mkdir -p /var/lib/rancher/k3s/agent/etc/containerd/

COPY config.toml.tmpl /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl

# Deploy the nvidia driver plugin on startup
RUN mkdir -p /var/lib/rancher/k3s/server/manifests

COPY device-plugin-daemonset.yaml /var/lib/rancher/k3s/server/manifests/nvidia-device-plugin-daemonset.yaml

VOLUME /var/lib/kubelet
VOLUME /var/lib/rancher/k3s
VOLUME /var/lib/cni
VOLUME /var/log

ENV PATH=&quot;$PATH:/bin/aux&quot;

ENTRYPOINT [&quot;/bin/k3s&quot;]
CMD [&quot;agent&quot;]
</code></pre>
<ol>
<li>On the same directory, create the containerd configuration file, named <code>config.toml.tmpl</code> with the following content:</li>
</ol>
<pre><code>[plugins.opt]
  path = &quot;{{ .NodeConfig.Containerd.Opt }}&quot;

[plugins.cri]
  stream_server_address = &quot;127.0.0.1&quot;
  stream_server_port = &quot;10010&quot;

{{- if .IsRunningInUserNS }}
  disable_cgroup = true
  disable_apparmor = true
  restrict_oom_score_adj = true
{{end}}

{{- if .NodeConfig.AgentConfig.PauseImage }}
  sandbox_image = &quot;{{ .NodeConfig.AgentConfig.PauseImage }}&quot;
{{end}}

{{- if not .NodeConfig.NoFlannel }}
[plugins.cri.cni]
  bin_dir = &quot;{{ .NodeConfig.AgentConfig.CNIBinDir }}&quot;
  conf_dir = &quot;{{ .NodeConfig.AgentConfig.CNIConfDir }}&quot;
{{end}}

[plugins.cri.containerd.runtimes.runc]
  # ---- changed from 'io.containerd.runc.v2' for GPU support
  runtime_type = &quot;io.containerd.runtime.v1.linux&quot;

# ---- added for GPU support
[plugins.linux]
  runtime = &quot;nvidia-container-runtime&quot;

{{ if .PrivateRegistryConfig }}
{{ if .PrivateRegistryConfig.Mirrors }}
[plugins.cri.registry.mirrors]{{end}}
{{range $k, $v := .PrivateRegistryConfig.Mirrors }}
[plugins.cri.registry.mirrors.&quot;{{$k}}&quot;]
  endpoint = [{{range $i, $j := $v.Endpoints}}{{if $i}}, {{end}}{{printf &quot;%q&quot; .}}{{end}}]
{{end}}

{{range $k, $v := .PrivateRegistryConfig.Configs }}
{{ if $v.Auth }}
[plugins.cri.registry.configs.&quot;{{$k}}&quot;.auth]
  {{ if $v.Auth.Username }}username = &quot;{{ $v.Auth.Username }}&quot;{{end}}
  {{ if $v.Auth.Password }}password = &quot;{{ $v.Auth.Password }}&quot;{{end}}
  {{ if $v.Auth.Auth }}auth = &quot;{{ $v.Auth.Auth }}&quot;{{end}}
  {{ if $v.Auth.IdentityToken }}identitytoken = &quot;{{ $v.Auth.IdentityToken }}&quot;{{end}}
{{end}}
{{ if $v.TLS }}
[plugins.cri.registry.configs.&quot;{{$k}}&quot;.tls]
  {{ if $v.TLS.CAFile }}ca_file = &quot;{{ $v.TLS.CAFile }}&quot;{{end}}
  {{ if $v.TLS.CertFile }}cert_file = &quot;{{ $v.TLS.CertFile }}&quot;{{end}}
  {{ if $v.TLS.KeyFile }}key_file = &quot;{{ $v.TLS.KeyFile }}&quot;{{end}}
{{end}}
{{end}}
{{end}}
</code></pre>
<ol>
<li>Finally, create the <code>device-plugin-daemonset.yaml</code> file with the following content.</li>
</ol>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      nodeSelector:
        accelerator: gpu
      priorityClassName: &quot;system-node-critical&quot;
      containers:
      - image: nvidia/k8s-device-plugin:1.0.0-beta4
        name: nvidia-device-plugin-ctr
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: [&quot;ALL&quot;]
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
</code></pre>
<p>Note that the daemonset definition includes a <code>nodeSelector</code> to ensure that it is only scheduled on nodes with the <code>accelerator=gpu</code> label.
The NVIDIA device plugin is a daemonset that exposes the number of GPUs available on the nodes, keep track of the GPUs health and enable running GPU enabled containers.</p>
<ol>
<li>With all those files on the same directory, build the image running the following command:</li>
</ol>
<pre><code class="language-bash">$ docker build . -t k3s:v1.21.5-k3s1-cuda --build-arg CUDA_VERSION=X.X.X
</code></pre>
<p>Replace <code>X.X.X</code> with the latest cuda version supported by your GPU.</p>
<ol>
<li>Test the built image by running the following command to create a cluster and deploy a test pod:</li>
</ol>
<pre><code class="language-bash">$ k3d cluster create gputest --image=k3s:v1.21.5-k3s1-cuda --gpus=1 --k3s-node-label &quot;accelerator=gpu@server:0&quot;
$ cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: cuda-vector-add
spec:
  restartPolicy: OnFailure
  containers:
    - name: cuda-vector-add
      image: &quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;
      resources:
        limits:
          nvidia.com/gpu: 1
EOF
</code></pre>
<p>Wait for the pod to finish running and check its logs, it should output something like:</p>
<pre><code class="language-bash">$ kubectl logs cuda-vector-add
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
</code></pre>
<p>If the pod is stuck in <code>Pending</code> state, check the logs from the pod created by NVIDIA device plugin daemonset.
If it succeeds you can delete the test cluster by running the following command:</p>
<pre><code class="language-bash">$ k3d cluster delete gputest
</code></pre>
<h4 id="create-the-kubernetes-cluster">Create the Kubernetes cluster</h4>
<p>Run the following command to create the cluster with 2 nodes and label one node with <code>accelerator=gpu</code>, we also disable traefik and expose the http port to allow accessing the dashboards from FuseML extensions:</p>
<pre><code class="language-bash">$ k3d cluster create fuseml --image=k3s:v1.21.5-k3s1-cuda --gpus=1 --agents 2 --k3s-node-label 'accelerator=gpu@agent:0'  --k3s-arg '--disable=traefik@server:0' -p '80:80@loadbalancer'
</code></pre>
<h4 id="install-fuseml">Install FuseML</h4>
<ol>
<li>Get the fuseml-installer:</li>
</ol>
<pre><code class="language-bash">$ curl -sfL https://raw.githubusercontent.com/fuseml/fuseml/main/install.sh | bash
Verifying checksum... Done.
Preparing to install fuseml-installer into /usr/local/bin
fuseml-installer installed into /usr/local/bin/fuseml-installer
Run 'fuseml-installer --help' to see what you can do with it.
</code></pre>
<p>Make sure you have <code>kubectl</code> and <code>helm</code> installed. If not, refer to the following links to install them:
    * kubectl: https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/
    * helm: https://helm.sh/docs/intro/install/</p>
<ol>
<li>To install FuseML run the following command, note that we are also installing the mlflow and kfserving extensions which will be used for model tracking and serving:</li>
</ol>
<pre><code class="language-bash">$ fuseml-installer install --system-domain &lt;MY_IP&gt;.nip.io --extensions mlflow,kfserving
</code></pre>
<p>Replace <code>MY_IP</code> with the IP address of the machine where the kubernetes cluster is running.</p>
<h2 id="validating-the-model">Validating the model</h2>
<p>As GPU is usually a limited and costly resource, instead of testing/developing the model using a GPU it might make sense to validate the model first on CPU. To do that we will simply create a FuseML workflow that will train the model and serve it on CPU. This will also enable us to compare CPU and GPU performance during the training.</p>
<p>For that experiment we will be training a <a href="https://developers.google.com/machine-learning/glossary/#convolutional_neural_network">Convolutional Neural Network (CNN)</a> to classify <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR images</a> using the Keras Sequential API. The complete code for model training is available <a href="https://github.com/fuseml/examples/tree/main/codesets/mlflow/keras">here</a>.</p>
<p>The following steps describe how to use FuseML to train the model and serve it.
1. Clone the fuseml/examples repository and register the <code>keras</code> example code as a FuseML codeset:</p>
<pre><code class="language-bash">$ git clone https://github.com/fuseml/examples.git
$ fuseml codeset register -n cifar10 -p demo examples/codesets/mlflow/keras
Pushing the code to the git repository...
Codeset http://gitea.192.168.86.74.nip.io/demo/cifar10.git successfully registered
</code></pre>
<ol>
<li>Create a FuseML workflow, note that the workflow includes steps for training and serving the trained model:</li>
</ol>
<pre><code class="language-bash">$ fuseml workflow create examples/workflows/mlflow-e2e.yaml
Workflow &quot;mlflow-e2e&quot; successfully created
</code></pre>
<ol>
<li>Assign the FuseML workflow to the <code>cifar10</code> codeset:</li>
</ol>
<pre><code class="language-bash">$ fuseml workflow assign -c cifar10 -p demo -n mlflow-e2e
Workflow &quot;mlflow-e2e&quot; assigned to codeset &quot;demo/cifar10&quot;
</code></pre>
<ol>
<li>Wait for the workflow run to finish running:
By assigning the workflow to the codeset, a workflow run will be created. You can check the status of the workflow run by running the following command:</li>
</ol>
<pre><code class="language-bash">$ fuseml workflow list-runs
+---------------------------+------------+----------------+----------+---------+
| NAME                      | WORKFLOW   | STARTED        | DURATION | STATUS  |
+---------------------------+------------+----------------+----------+---------+
| fuseml-demo-cifar10-tkgls | mlflow-e2e | 14 seconds ago | ---      | Running |
+---------------------------+------------+----------------+----------+---------+
</code></pre>
<p>You can also see a more detailed view of the workflow run through the tekton dashboard, which should be available at: http://tekton.<MY_IP>.nip.io
Note that since this is the first time the workflow is running, it will build a docker image including the dependencies for training the model which may take a while. However, consecutive runs will skip that step as long as the dependencies did not change.</p>
<ol>
<li>Validate the deployed model:
Before querying the served model for predictions, take a look at MLflow for detailed information about the model, such as its accuracy, loss, parameters, etc. MLflow should be available at http://mlflow.<MY_IP>.nip.io. For example:</li>
</ol>
<p><img alt="MLflow" src="../img/triton-basic-mlflow.png" /></p>
<p>Note the accuracy (about 70%) and the training duration (about 4 minutes) for 10 epochs.</p>
<p>With the successful execution of the workflow, a new FuseML application should have been created. List the FuseML applications:</p>
<pre><code class="language-bash">$ fuseml application list
+--------------+-----------+----------------------------------------------+----------------------------------------------------------------------------------------+------------+
| NAME         | TYPE      | DESCRIPTION                                  | URL                                                                                    | WORKFLOW   |
+--------------+-----------+----------------------------------------------+----------------------------------------------------------------------------------------+------------+
| demo-cifar10 | predictor | Application generated by mlflow-e2e workflow | http://demo-cifar10.fuseml-workloads.192.168.86.74.nip.io/v2/models/demo-cifar10/infer | mlflow-e2e |
+--------------+-----------+----------------------------------------------+----------------------------------------------------------------------------------------+------------+
</code></pre>
<p>The list of FuseML applications include a URL to query the model for predictions. As we are using the kfserving extension for deploying the model, you can check the deployed models through the kfserving dashboard: http://kfserving.<MY_IP>.nip.io which also includes more detailed information such as the status of the deployment, logs, etc. For example:</p>
<p><img alt="KSserve Dashboard" src="../img/triton-basic-kserve.png" /></p>
<p>To validate the model, we need to send a request containing an image so the model can to predict its class.
The json file included in the <code>fuseml/examples</code> repository contains a sample request that includes the following deer image:</p>
<p><img alt="Deer" src="../img/triton-basic-deer.png" /></p>
<p>Run the following command to send the request to the application URL:</p>
<pre><code class="language-bash">$ curl -sX POST http://demo-cifar10.fuseml-workloads.192.168.86.74.nip.io/v2/models/demo-cifar10/infer -d @examples/prediction/data-keras.json | jq
{
  &quot;model_name&quot;: &quot;demo-cifar10&quot;,
  &quot;model_version&quot;: &quot;1&quot;,
  &quot;outputs&quot;: [
    {
      &quot;name&quot;: &quot;dense_1&quot;,
      &quot;datatype&quot;: &quot;FP32&quot;,
      &quot;shape&quot;: [
        1,
        10
      ],
      &quot;data&quot;: [
        -2.26723575592041,
        -7.539040565490723,
        1.4853938817977905,
        1.297321081161499,
        4.158736705780029,
        2.9821133613586426,
        -2.7044689655303955,
        3.2879271507263184,
        -5.1592817306518555,
        -4.101395130157471
      ]
    }
  ]
}
</code></pre>
<p>To be able to interpret the predictions, we need to know the classes indexes meaning. The following is how the classes are indexed:</p>
<pre><code>0: 'airplane'
1: 'automobile'
2: 'bird'
3: 'cat'
4: 'deer'
5: 'dog'
6: 'frog'
7: 'horse'
8: 'ship'
9: 'truck'
</code></pre>
<p>With that information, we can see that the model correctly predicted that the image is a deer (higher number on index 4).</p>
<h2 id="training-and-serving-the-model-on-gpu">Training and serving the model on GPU</h2>
<p>Now that the model is validated, we can proceed to train and serve the model with higher accuracy on GPU.</p>
<ol>
<li>
<p>Update the example workflow so that it will train and serve the model on GPU:
Open the file <code>examples/workflows/mlflow-e2e.yaml</code> with your favorite text editor and make the following changes:</p>
<p>Change the workflow name to <code>mlflow-e2e-gpu</code>.</p>
<p>Add the following snippet under the <code>trainer</code> step:
    <code>yaml
        - name: "trainer"
        ...
        resources:
          limits:
            nvidia.com/gpu: 1</code>
    This will ensure that the trainer pod will be scheduled to a node that has a GPU.</p>
<p>For the <code>predictor</code> step, we need to add the following input:
    <code>yaml
    - name: predictor
    ...
    inputs:
        ...
        - name: resources_limits
        value: '{nvidia.com/gpu: 1}'</code>
    This ensures that the workload (KFserving inference service) created by this step will be scheduled to a node that has a GPU.</p>
</li>
<li>
<p>Unassign the <code>mlflow-e2e</code> workflow from the <code>cifar10</code>:</p>
</li>
</ol>
<pre><code class="language-bash">$ fuseml workflow unassign -c cifar10 -p demo -n mlflow-e2e
Workflow &quot;mlflow-e2e&quot; unassigned from codeset &quot;demo/cifar10&quot;
</code></pre>
<ol>
<li>Increase the number of epochs on the <code>keras</code> example codeset to 60, so we get a more accurate model:</li>
</ol>
<pre><code class="language-bash">$ sed -i 's/10/60/' examples/codesets/mlflow/keras/MLproject
$ fuseml codeset register -n cifar10 -p demo examples/codesets/mlflow/keras
Pushing the code to the git repository...
Codeset http://gitea.192.168.86.74.nip.io/demo/cifar10.git successfully registered
</code></pre>
<ol>
<li>Create the <code>mlflow-e2e-gpu</code> workflow and assign it to the <code>cifar10</code> codeset:</li>
</ol>
<pre><code class="language-bash">$ fuseml workflow create examples/workflows/mlflow-e2e.yaml
Workflow &quot;mlflow-e2e-gpu&quot; successfully created

$ fuseml workflow assign -c cifar10 -p demo -n mlflow-e2e-gpu
Workflow &quot;mlflow-e2e-gpu&quot; assigned to codeset &quot;demo/cifar10&quot;
</code></pre>
<ol>
<li>Wait for the workflow run to finish running:
You can check the status of the workflow run by running the following command:</li>
</ol>
<pre><code class="language-bash">$ fuseml workflow list-runs
+---------------------------+----------------+----------------+------------+-----------+
| NAME                      | WORKFLOW       | STARTED        | DURATION   | STATUS    |
+---------------------------+----------------+----------------+------------+-----------+
| fuseml-demo-cifar10-pld47 | mlflow-e2e-gpu | 8 seconds ago  | ---        | Running   |
| fuseml-demo-cifar10-tkgls | mlflow-e2e     | 23 minutes ago | 12 minutes | Succeeded |
+---------------------------+----------------+----------------+------------+-----------+
</code></pre>
<p>If you head to the tekton dashboard (http://tekton.<LOCAL_IP>.nip.io) and look at the logs for the <code>trainer</code> step, you should be able to see a log confirming that the training is being performed on GPU, such as:</p>
<pre><code class="language-bash">Num GPUs: 1
1 Physical GPUs, 1 Logical GPUs
</code></pre>
<ol>
<li>Validate the trained model:
Now that we have the model trained, head over to MLflow (http://mlflow.<LOCAL_IP>.nip.io) to compare the training metrics. For example:</li>
</ol>
<p><img alt="MLflow" src="../img/triton-gpu-mlflow.png" /></p>
<p>We can see that now we have a model with accuracy of 96%! and although have have increased the number of epochs by 600%, we have spent basically the same amount of time on training.</p>
<p>You can also use MLflow to compare other metrics by selecting both runs and clicking on the <code>Compare</code> button.</p>
<p><img alt="MLflow Compare" src="../img/triton-gpu-compare.png" /></p>
<p>List the FuseML applications, and note that we have the same application however using the new model and also using the GPU.</p>
<pre><code class="language-bash">$ fuseml application list
+--------------+-----------+--------------------------------------------------+----------------------------------------------------------------------------------------+----------------+
| NAME         | TYPE      | DESCRIPTION                                      | URL                                                                                    | WORKFLOW       |
+--------------+-----------+--------------------------------------------------+----------------------------------------------------------------------------------------+----------------+
| demo-cifar10 | predictor | Application generated by mlflow-e2e-gpu workflow | http://demo-cifar10.fuseml-workloads.192.168.86.74.nip.io/v2/models/demo-cifar10/infer | mlflow-e2e-gpu |
+--------------+-----------+--------------------------------------------------+----------------------------------------------------------------------------------------+----------------+
</code></pre>
<p>To confirm that the serving is using the GPU, you can check the serving logs at http://kfserving-models-web-app.<LOCAL_IP>.nip.io/details/fuseml-workloads/demo-cifar10, it should have something like:</p>
<pre><code class="language-bash">I1019 12:58:00.349814 1 metrics.cc:290] Collecting metrics for GPU 0: NVIDIA XXX
</code></pre>
<p>Now, just like before, send a request to the application URL.</p>
<pre><code class="language-bash">$ curl -sX POST http://demo-cifar10.fuseml-workloads.192.168.86.74.nip.io/v2/models/demo-cifar10/infer -d @examples/prediction/data-keras.json | jq
{
  &quot;model_name&quot;: &quot;demo-cifar10&quot;,
  &quot;model_version&quot;: &quot;1&quot;,
  &quot;outputs&quot;: [
    {
      &quot;name&quot;: &quot;dense_1&quot;,
      &quot;datatype&quot;: &quot;FP32&quot;,
      &quot;shape&quot;: [
        1,
        10
      ],
      &quot;data&quot;: [
        -9.919892311096191,
        -17.90753936767578,
        4.234504699707031,
        2.73684024810791,
        15.640728950500488,
        4.6014084815979,
        -1.7354743480682373,
        10.520237922668457,
        -6.34201717376709,
        -10.711426734924316
      ]
    }
  ]
}
</code></pre>
<p>Once again we can confirm that the model correctly predicted that the image is a deer (higher number on index 4).</p>
<h3 id="cleanup">Cleanup</h3>
<p>Delete the cluster by running the following command:</p>
<pre><code class="language-bash">$ k3d cluster delete fuseml
</code></pre>
<p>The docker containers created by k3d will be deleted together with all workloads.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this tutorial we have demonstrated how FuseML can be used to validate, train and serve ML models on GPU. Additionally, we have demonstrated the use of GPU for inference and training models quicker. Although the model used in this tutorial is not complex, meaning that it could have been trained without GPU, it is a good example that enabled us to see how using a GPU can speed up the training process.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../kfserving-basic/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Logistic Regression with MLFlow &amp; KFServing" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Logistic Regression with MLFlow & KFServing
            </div>
          </div>
        </a>
      
      
        
        <a href="../../architecture/" class="md-footer__link md-footer__link--next" aria-label="Next: Architecture" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Architecture
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2021 FuseML Author(s)
          </div>
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.8397ff9e.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.1e84347e.min.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
    
  </body>
</html>